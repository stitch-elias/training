import torch
from torch import nn
import torch.nn.functional as F

class Mish(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self,x):
        x = x * (torch.tanh(F.softplus(x)))
        return x

class AdaptiveAvgPool(nn.Module):
    def __init__(self,out_s,type="Max"):
        super(AdaptiveAvgPool, self).__init__()
        self.out_s = out_s
        self.type = type
    def forward(self,x):
        in_size = x.shape[1:]
        out_size = torch.tensor(self.out_s)

        str = torch.floor_(in_size/out_size)
        kernel = in_size-(out_size-1)*str

        if self.type == 'Avg':
            pool = nn.AvgPool2d(kernel_size=kernel,stride=(kernel[0].item(),kernel[1].item()))
        else:
            pool = nn.MaxPool2d(kernel_size=kernel,stride=(kernel[0].item(),kernel[1].item()))
        return pool(x)

class SE(nn.Module):
    def __init__(self,in_c,ratio):
        super(SE, self).__init__()
        self.squeeze = AdaptiveAvgPool((1, 1))
        self.compress = nn.Conv2d(in_c, in_c // ratio, 1, 1, 0)
        self.excitation = nn.Conv2d(in_c // ratio, in_c, 1, 1, 0)

    def forward(self, x):
        out = self.squeeze(x)
        out = self.compress(out)
        out = F.relu(out)
        out = self.excitation(out)
        return F.sigmoid(out)

class Conv_bn_mish(nn.Module):
    def __init__(self,in_c,out_c,kernel=3,str=1,pad=1,is_se=False,bias=True,is_shortcat=False):
        super(Conv_bn_mish, self).__init__()
        self.is_se = is_se
        self.is_shortcat = is_shortcat

        self.Conv = nn.Conv2d(in_channels=in_c,out_channels=out_c,kernel_size=kernel,stride=str,padding=pad,bias=bias)
        self.bn = nn.BatchNorm2d(out_c)
        self.mish = Mish()

        if self.is_se:
            self.se = SE(out_c,16)

        if self.is_shortcat:
            self.short_cat = nn.Sequential()

            if str != 1:
                self.short_cut = nn.Sequential(
                    nn.Conv2d(in_c, out_c, 1, stride=str, padding=0, bias=False),
                    nn.BatchNorm2d(out_c)
                )

    def forward(self,x):
        out = self.Conv(x)
        out = self.bn(out)
        out = self.mish(out)

        if self.is_se:
            coefficient = self.se(out)
            out *= coefficient
        if self.is_shortcat:
            out += self.short_cut(x)
        return out


class res_basic_block(nn.Module):
    def __init__(self, in_c, out_c, kernel=3, str=2, pad=1, is_se=False, bias=False):
        super(res_basic_block, self).__init__()
        self.is_se = is_se

        self.Conv1 = nn.Conv2d(in_channels=in_c, out_channels=out_c, kernel_size=kernel, stride=str, padding=pad,
                              bias=bias)
        self.Conv2 = nn.Conv2d(in_channels=out_c, out_channels=out_c, kernel_size=kernel, stride=1, padding=pad,
                              bias=bias)
        self.bn = nn.BatchNorm2d(out_c)
        self.mish = Mish()

        if self.is_se:
            self.se = SE(out_c, 16)


        self.short_cat = nn.Sequential()

        if str != 1:
            self.short_cut = nn.Sequential(
                nn.Conv2d(in_c, out_c, 1, stride=str, padding=0, bias=False),
                nn.BatchNorm2d(out_c)
            )

    def forward(self, x):
        out = self.Conv1(x)
        out = self.bn(out)
        out = self.mish(out)

        out = self.Conv2(out)
        out = self.bn(out)

        if self.is_se:
            coefficient = self.se(out)
            out *= coefficient

        out += self.short_cut(x)
        return self.mish(out)

class res_bottle_neck(nn.Module):
    def __init__(self, in_channels, out_channels, strides, is_se=False, bias=False):
        super(res_bottle_neck, self).__init__()
        self.is_se = is_se
        self.mish = Mish()
        self.conv1 = nn.Conv2d(in_channels, out_channels//4, 1, stride=1, padding=0, bias=bias)
        self.conv2 = nn.Conv2d(out_channels//4, out_channels//4, 3, stride=strides, padding=1, bias=bias)
        self.conv3 = nn.Conv2d(out_channels//4, out_channels, 1, stride=1, padding=0, bias=bias)
        self.bn1 = nn.BatchNorm2d(out_channels//4)
        self.bn2 = nn.BatchNorm2d(out_channels)
        if self.is_se:
            self.se = SE(out_channels, 16)

        self.shortcut = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, stride=strides, padding=0, bias=bias),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.mish(out)
        out = self.conv2(out)
        out = self.bn1(out)
        out = self.mish(out)
        out = self.conv3(out)
        out = self.bn2(out)
        if self.is_se:
            coefficient = self.se(out)
            out *= coefficient
        out += self.shortcut(x)
        return self.mish(out)

class StemBlock(nn.Module):
    def __init__(self, inp=3, num_init_features=32):
        super(StemBlock, self).__init__()

        self.stem_1 = Conv_bn_mish(inp, num_init_features, 3, 1, 1)
        self.stem_2a = Conv_bn_mish(num_init_features, int(num_init_features / 2), 1, 1, 0)
        self.stem_2b = Conv_bn_mish(int(num_init_features / 2), num_init_features, 3, 2, 1)
        self.stem_2p = nn.MaxPool2d(kernel_size=2, stride=2)
        self.stem_3 = Conv_bn_mish(num_init_features * 2, num_init_features, 1, 1, 0)

    def forward(self, x):
        stem_1_out = self.stem_1(x)

        stem_2a_out = self.stem_2a(stem_1_out)
        stem_2b_out = self.stem_2b(stem_2a_out)

        stem_2p_out = self.stem_2p(stem_1_out)

        out = self.stem_3(torch.cat((stem_2b_out, stem_2p_out), 1))
        return out

class rearrange(nn.Module):
    def __init__(self):
        super(rearrange, self).__init__()
    def forward(self,x):
        x = torch.transpose(x,dim0=1,dim1=3)
        x = torch.transpose(x,dim0=1,dim1=2)
        x = x.reshape(x.shape[0],-1,x.shape[-1])
        return x

class attention(nn.Module):
    def __init__(self,dim,heads=8,dim_head=64,dropout=0.):
        super(attention, self).__init__()
        inner_dim = dim_head*heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim=-1)
        self.to_qkv = nn.Linear(dim,inner_dim*3,bias=False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim,dim),
            nn.Dropout(dropout),
        ) if project_out else nn.Identity()

    def forward(self,x):
        b,n,_ = x.shape
        h = self.heads
        qkv = self.to_qkv(x)
        qkv = qkv.chunk(3,dim=-1)
        q,k,v = map(lambda i :torch.transpose(i.reshape(b,n,h,-1),1,2),qkv)

        dots = torch.matmul(q,torch.transpose(k,2,3))*self.scale

        attn = self.attend(dots)
        out = torch.matmul(attn,v)
        out = torch.transpose(out,1,2).reshape(b,n,-1)
        return out

class feedforward(nn.Module):
    def __init__(self,dim,hidden_dim,dropout=0.):
        super(feedforward, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(dim,hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim,dim),
            nn.Dropout(dropout)
        )
    def forward(self,x):
        return self.net(x)

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

class transformerBlock(nn.Module):
    def __init__(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):
        super(transformerBlock, self).__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),
                PreNorm(dim, feedforward(dim, mlp_dim, dropout=dropout))
            ]))

    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x


class vit(nn.Module):
    def __init__(self,input_size,patch_size,out_dim,dim,depth,heads,mlp_dim,pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):
        super(vit, self).__init__()
        image_height, image_width = self.pair(input_size)
        patch_height, patch_width = self.pair(patch_size)

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        # patch_dim = channels * patch_height * patch_width

        self.to_patch_embedding = nn.Sequential(
            # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),
            # nn.Linear(patch_dim, dim)
            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size),
            rearrange()
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = transformerBlock(dim, depth, heads, dim_head, mlp_dim, dropout)

        self.pool = pool
        self.to_latent = nn.Identity()

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, out_dim)
        )

    def forward(self, img):
        x = self.to_patch_embedding(img)  # b c (h p1) (w p2) -> b (h w) (p1 p2 c) -> b (h w) dim
        b, n, _ = x.shape  # b表示batchSize, n表示每个块的空间分辨率, _表示一个块内有多少个值

        cls_tokens = self.cls_token.repeat(b,1,1)# self.cls_token: (1, 1, dim) -> cls_tokens: (batchSize, 1, dim)

        x = torch.cat((cls_tokens, x), dim=1)  # 将cls_token拼接到patch token中去       (b, 65, dim)
        x += self.pos_embedding[:, :(n + 1)]  # 加位置嵌入（直接加）      (b, 65, dim)
        x = self.dropout(x)

        x = self.transformer(x)  # (b, 65, dim)

        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]  # (b, dim)

        x = self.to_latent(x)  # Identity (b, dim)

        return self.mlp_head(x)

    def pair(self,t):
        return t if isinstance(t, tuple) else (t, t)
class trt_vit_block(nn.Module):
    def __init__(self,c_in,c_out,r,dim_head):
        super(trt_vit_block, self).__init__()
        self.pool = nn.AvgPool2d(kernel_size=3,stride=2,padding=1)
        self.conv = nn.Conv2d(c_in,int(c_out*r),kernel_size=1,stride=1)

        self.tran = transformerBlock(int(c_out*r),depth=1,heads=int(c_out*r)//dim_head,dim_head=dim_head,mlp_dim=(c_out-int(c_out*r)))
        self.bottle = res_bottle_neck((c_out-int(c_out*r)),c_out,1)

    def forward(self,x):
        out = self.pool(x)
        out = self.conv(out)
        h,w = out.shape[-2:]
        out = torch.transpose(out.reshape(out.shape[0],out.shape[1],-1),dim0=1,dim1=2)
        out = self.tran(out)
        out = torch.transpose(out,dim0=1,dim1=2)
        out = out.reshape(out.shape[0],out.shape[1],h,w)
        out = self.bottle(out)
        return out

class trt_VIT(nn.Module):
    def __init__(self,classes):
        super(trt_VIT, self).__init__()
        self.stem = StemBlock(3,32)
        self.stage1 = res_basic_block(32,64,3,2,1)
        self.stage2 = nn.Sequential(
            res_bottle_neck(64,64,1),
            res_bottle_neck(64, 128, 2),
        )
        self.stage3 = nn.Sequential(
            res_bottle_neck(128,128,1),
            res_bottle_neck(128, 128, 1),
            res_bottle_neck(128, 256, 1),
            res_bottle_neck(256, 256, 2),
        )
        self.stage4 = nn.Sequential(
            res_bottle_neck(256,256,1),
            res_bottle_neck(256, 256, 1),
            res_bottle_neck(256, 512, 1),
            res_bottle_neck(512, 512, 1),
            res_bottle_neck(512, 512, 2),
        )

        self.t = nn.Sequential(
            trt_vit_block(512,512,0.5,4),
            trt_vit_block(512, 512, 0.5,4),
            trt_vit_block(512, 1024, 0.5,4),
            trt_vit_block(1024, classes, 0.5,1),
        )

    def forward(self,x):
        out = self.stem(x)
        out = self.stage1(out)
        out = self.stage2(out)
        out = self.stage3(out)
        out = self.stage4(out)
        out = self.t(out)
        return out

net = trt_VIT(10)
x = torch.randn((10,3,128,128))
pred = net(x)
print(pred.shape)

# model_vit = vit(
#         input_size= 256,
#         patch_size = 32,
#         out_dim = 1000,
#         dim = 1024,
#         depth = 6,
#         heads = 16,
#         mlp_dim = 2048,
#         dropout = 0.1,
#         emb_dropout = 0.1
#     )
#
# img = torch.randn(16, 3, 256, 256)
#
# preds = model_vit(img)
#
# print(preds.shape)  # (16, 1000)
